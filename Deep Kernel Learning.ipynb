{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7122c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import joblib\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "051640f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aluminium_inputs = joblib.load('../Vectors/single_f_aluminium_vectors')\n",
    "aluminium_labels = joblib.load('../Vectors/single_f_aluminium_labels')\n",
    "aluminium_dates = joblib.load('../Vectors/single_f_aluminium_dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe60f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches,validation_batches,test_batches = utils.train_test_split(aluminium_inputs,aluminium_labels,aluminium_dates,aluminium_inputs.shape[0],0.8,0.7,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfbdf0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_labels,train_dates = train_batches[0]\n",
    "test_data,test_labels,test_dates = test_batches[0]\n",
    "val_data,val_labels,val_dates = validation_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb7fa056",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.from_numpy(train_data)\n",
    "train_labels = torch.from_numpy(train_labels)\n",
    "test_data = torch.from_numpy(test_data)\n",
    "test_labels = torch.from_numpy(test_labels)\n",
    "val_data = torch.from_numpy(val_data)\n",
    "val_labels = torch.from_numpy(val_labels)\n",
    "train_data = torch.squeeze(train_data, dim = 1)\n",
    "test_data = torch.squeeze(test_data, dim = 1)\n",
    "val_data = torch.squeeze(val_data, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6217881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([672, 55])\n",
      "torch.Size([672])\n",
      "torch.Size([50, 55])\n",
      "torch.Size([50])\n",
      "torch.Size([118, 55])\n",
      "torch.Size([118])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcdbc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f21f8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_data.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 100))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(100, 20))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(20, representation_dim))\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18534ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=representation_dim)),\n",
    "                num_dims=representation_dim, grid_size=100\n",
    "            )\n",
    "            '''self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())'''\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            # We're also scaling the features so that they're nice values\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = projected_x - projected_x.min(0)[0]\n",
    "            projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c47efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''noise_prior = gpytorch.priors.GammaPrior(1.1, 0.05)\n",
    "noise_prior_mode = (noise_prior.concentration - 1) / noise_prior.rate\n",
    "MIN_INFERRED_NOISE_LEVEL = 1e-5\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "    noise_prior=noise_prior,\n",
    "    noise_constraint=gpytorch.constraints.Interval(\n",
    "        MIN_INFERRED_NOISE_LEVEL,\n",
    "        transform=None,\n",
    "        initial_value=noise_prior_mode,\n",
    "        upper_bound = 10\n",
    "    ),\n",
    ")'''\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_data,train_labels, likelihood).double()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e491c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ea3e7f217349079f5f744a1f96fc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 200\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "def train():\n",
    "    iterator = tqdm.notebook.tqdm(range(training_iterations))\n",
    "    for i in iterator:\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_data)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_labels)\n",
    "        training_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get into evaluation (predictive posterior) mode\n",
    "        '''model.eval()\n",
    "        likelihood.eval()\n",
    "                \n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var(), gpytorch.settings.use_toeplitz(False):\n",
    "            ## Calculate the validation loss\n",
    "            val_loss = -mll(model(val_data), val_labels)\n",
    "            validation_loss.append(val_loss.item())'''\n",
    "        \n",
    "    index = [i+1 for i in range(len(training_loss))]\n",
    "    plt.plot(index, training_loss, label = \"train loss\")\n",
    "    #plt.plot(index, validation_loss, label = \"val loss\")\n",
    "    plt.xlabel('Iters')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e3958",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "train_predictions = []\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    train_predictions.append(likelihood(model(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_means = []\n",
    "train_pred_uconf = []\n",
    "train_pred_lconf = []\n",
    "\n",
    "for p in train_predictions:\n",
    "    train_pred_means.append(p.mean.numpy())\n",
    "    l, u = p.confidence_region()\n",
    "    train_pred_lconf.append(l.detach().numpy())\n",
    "    train_pred_uconf.append(u.detach().numpy())\n",
    "    \n",
    "train_pred_means = train_pred_means[0]\n",
    "train_pred_lconf = train_pred_lconf[0]\n",
    "train_pred_uconf = train_pred_uconf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i+1 for i in range(len(train_pred_means[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(index,train_pred_means,label = 'train predictions')\n",
    "ax.plot(index,train_labels,label = 'train predictions')\n",
    "ax.fill_between(index, train_pred_lconf, train_pred_uconf, color='r', alpha=.005)\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Training Learning')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc56bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30e7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
