{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import joblib\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = joblib.load('../Vectors/aluminium_vectors')\n",
    "labels = joblib.load('../Vectors/aluminium_labels')\n",
    "dates = joblib.load('../Vectors/dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(969, 7, 30)\n",
      "(969,)\n",
      "(969,)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "print(dates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches,validation_batches,test_batches = utils.train_test_split(inputs,labels,dates,inputs.shape[0],0.8,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dates: 2017-05-25 2020-05-13\n",
      "Val Dates: 2020-05-14 2020-09-25\n",
      "Test Dates: 2020-09-28 2021-02-09\n"
     ]
    }
   ],
   "source": [
    "train_data,train_labels,train_dates = train_batches[0]\n",
    "test_data,test_labels,test_dates = test_batches[0]\n",
    "val_data,val_labels,val_dates = validation_batches[0]\n",
    "print(\"Train Dates: \" + train_dates[0] + \" \" + train_dates[-1])\n",
    "print(\"Val Dates: \" + val_dates[0] + \" \" + val_dates[-1])\n",
    "print(\"Test Dates: \" + test_dates[0] + \" \" + test_dates[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.from_numpy(train_data).to(dtype=torch.float16).to(device)\n",
    "train_labels = torch.from_numpy(train_labels).to(device)\n",
    "test_data = torch.from_numpy(test_data).to(dtype=torch.float16).to(device)\n",
    "test_labels = torch.from_numpy(test_labels).to(device)\n",
    "val_data = torch.from_numpy(val_data).to(dtype=torch.float16).to(device)\n",
    "val_labels = torch.from_numpy(val_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "test_labels = torch.reshape(test_labels, (test_labels.shape[0], 1))\n",
    "val_labels = torch.reshape(val_labels, (val_labels.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([775, 7, 30])\n",
      "torch.Size([775, 1])\n",
      "torch.Size([97, 7, 30])\n",
      "torch.Size([97, 1])\n",
      "torch.Size([97, 7, 30])\n",
      "torch.Size([97, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a dictionary with the parameters of the Neural Network\n",
    "params_dictionary = {}\n",
    "params_dictionary['embedding_size'] = train_data[0].shape[1]\n",
    "params_dictionary['hidden_dim'] = 256\n",
    "params_dictionary['linear_dims'] = [64,32]\n",
    "params_dictionary['bidirectional'] = True\n",
    "params_dictionary['label_size'] = 1\n",
    "params_dictionary['dropout'] = 0.3\n",
    "params_dictionary['rnn_layers_num'] = 2\n",
    "params_dictionary['attention_layer'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, device,hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.concat_linear = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        ## add this model the same same device with the RNN\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, rnn_outputs, final_hidden_state):\n",
    "        attn_weights = self.attn(rnn_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "        attn_weights = torch.bmm(attn_weights, final_hidden_state.unsqueeze(2))\n",
    "        attn_weights = F.softmax(attn_weights.squeeze(2), dim=1)\n",
    "        context = torch.bmm(rnn_outputs.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        attn_hidden = torch.tanh(self.concat_linear(torch.cat((context, final_hidden_state), dim=1)))\n",
    "        return attn_hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnRegressor(nn.Module):\n",
    "    def __init__(self, device, params_dictionary):\n",
    "        super(RnnRegressor, self).__init__()\n",
    "        self.params = params_dictionary\n",
    "        self.device = device\n",
    "\n",
    "        # Calculate number of directions\n",
    "        self.num_directions = 2 if self.params.get('bidirectional') == True else 1\n",
    "\n",
    "        # define an attention model\n",
    "        # Choose attention model\n",
    "        self.attention = Attention(self.device,self.params.get('hidden_dim')* self.num_directions)\n",
    "\n",
    "        ## here store in a list all the dimensions of the layers rnn_output --> linear layers --> labels layer\n",
    "        self.linear_dims = [self.params.get('hidden_dim') * self.num_directions] + self.params.get('linear_dims')\n",
    "        self.linear_dims.append(self.params.get('label_size'))\n",
    "\n",
    "        # Work with LSTM cell for now\n",
    "        self.rnn = nn.LSTM\n",
    "\n",
    "        ## define the RNN layer\n",
    "        self.rnn = self.rnn(self.params.get('embedding_size'),\n",
    "                            self.params.get('hidden_dim'),\n",
    "                            num_layers=self.params.get('rnn_layers_num'),\n",
    "                            bidirectional=self.params.get('bidirectional'),\n",
    "                            dropout=float(self.params.get('dropout')),\n",
    "                            batch_first=True)\n",
    "        \n",
    "        ## the hidden state of the RNN empty for now\n",
    "        self.hidden = None\n",
    "        \n",
    "        # Define set of fully connected layers (Linear Layer + Activation Layer)\n",
    "        ## this set of layers takes the output of the RNN or the Attention layer and applies a feedforward NN on it\n",
    "        ## consecutive linear + Relu layers are applied (the final layer does not have a relu activation!)\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(0, len(self.linear_dims)-1):\n",
    "            if self.params.get('dropout') > 0.0:\n",
    "                self.linears.append(nn.Dropout(p=self.params.get('dropout')))\n",
    "            linear_layer = nn.Linear(self.linear_dims[i], self.linear_dims[i+1])\n",
    "            self.init_weights(linear_layer)\n",
    "            self.linears.append(linear_layer)\n",
    "            if i == len(self.linear_dims) - 1:\n",
    "                break\n",
    "            self.linears.append(nn.ReLU())\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(0.01)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.params.get('rnn_layers_num') * self.num_directions, batch_size, self.params.get('hidden_dim')).to(self.device),\n",
    "              torch.zeros(self.params.get('rnn_layers_num') * self.num_directions, batch_size, self.params.get('hidden_dim')).to(self.device))\n",
    "      \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, embedding_size = inputs.shape\n",
    "\n",
    "\n",
    "        embedded_inputs = inputs\n",
    "\n",
    "        ## initialise the hidden state of the RNN\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        ## pass the data through the recurrent layer\n",
    "        rnn_output, self.hidden = self.rnn(embedded_inputs.float(), self.hidden)\n",
    "\n",
    "        ## Collect last hidden state\n",
    "        final_state = self.hidden[0].view(self.params.get('rnn_layers_num'), self.num_directions, batch_size, self.params.get('hidden_dim'))[-1]\n",
    "\n",
    "        # Handle directions if more than one\n",
    "        final_hidden_state = None\n",
    "        ## in case we have only one direction\n",
    "        if self.num_directions == 1:\n",
    "            final_hidden_state = final_state.squeeze(0)\n",
    "        ## in case we have 2 directions concatenate these two states\n",
    "        elif self.num_directions == 2:\n",
    "            h_1, h_2 = final_state[0], final_state[1]\n",
    "            final_hidden_state = torch.cat((h_1, h_2), 1)  # Concatenate both states\n",
    "\n",
    "        ## Attention Layer\n",
    "        if(self.params.get('attention_layer') == False):\n",
    "            X = final_hidden_state\n",
    "        else:\n",
    "            rnn_output = rnn_output.permute(1, 0, 2)\n",
    "            X, attention_weights = self.attention(rnn_output, final_hidden_state)\n",
    "\n",
    "        # Push through linear layers\n",
    "        for l in self.linears:\n",
    "            X = l(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RnnRegressor(device,params_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer =  torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  9 0.7446760535240173\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss_batches = []\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model.forward(train_data)\n",
    "    single_loss = loss(y_pred,train_labels.float()).to(device)\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(single_loss.item())\n",
    "\n",
    "    ## compute and store the validation loss\n",
    "    valid_pred = model.forward(val_data)\n",
    "    valid_loss.append(loss(valid_pred,val_labels.float().to(device)))\n",
    "\n",
    "    print(\"epoch  \" + str(i+1) + \" \" + str(train_loss[-1]))\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
